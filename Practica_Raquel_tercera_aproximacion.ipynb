{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica_Raquel_tercera_aproximacion.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "35dBSz7SWkfp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "La idea inicial era aprovechar lo que hice en el segundo notebook, cargádolo de la siguiente forma:\n",
        "```\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "```\n",
        "Pero tarda más que si lo hiciera desde el principio, así que empezaré de nuevo. Voy a entrenar mi modelo con el conjunto de 6000 imágenes, a ver qué resultados obtengo."
      ]
    },
    {
      "metadata": {
        "id": "gp2yUoyDLAmT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c6369386-9d8d-4033-8e83-3571b2954754",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530258825137,
          "user_tz": -120,
          "elapsed": 98606,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_Dataset.zip\n",
        "!unzip -q Flickr8k_Dataset.zip\n",
        "!ls -la"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-29 07:52:07--  http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_Dataset.zip\n",
            "Resolving nlp.cs.illinois.edu (nlp.cs.illinois.edu)... 192.17.58.132\n",
            "Connecting to nlp.cs.illinois.edu (nlp.cs.illinois.edu)|192.17.58.132|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/zip]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi  65%[============>       ] 700.75M  6.40MB/s    eta 22s    "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Flickr8k_Dataset.zi 100%[===================>]   1.04G  14.0MB/s    in 84s     \n",
            "\n",
            "2018-06-29 07:53:32 (12.6 MB/s) - ‘Flickr8k_Dataset.zip’ saved [1115419746/1115419746]\n",
            "\n",
            "total 1089768\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 07:53 .\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 07:50 ..\n",
            "drwx------ 4 root root       4096 Jun 29 07:51 .cache\n",
            "drwxr-xr-x 3 root root       4096 Jun 29 07:51 .config\n",
            "drwxr-xr-x 3 root root       4096 Jun 25 16:59 datalab\n",
            "drwxr-xr-x 2 root root     450560 Oct  3  2012 Flicker8k_Dataset\n",
            "-rw-r--r-- 1 root root 1115419746 Oct 24  2013 Flickr8k_Dataset.zip\n",
            "drwxr-xr-x 4 root root       4096 Jun 29 07:51 .forever\n",
            "drwxr-xr-x 5 root root       4096 Jun 29 07:51 .ipython\n",
            "drwx------ 3 root root       4096 Jun 29 07:51 .local\n",
            "drwxrwxr-x 3 root root       4096 Oct 21  2013 __MACOSX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2kDz7pL2LCk8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "fd6a15db-9413-4091-8e14-a8f896f235ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530263831722,
          "user_tz": -120,
          "elapsed": 4986371,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        " \n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\t#print('>%s' % name)\n",
        "\treturn features\n",
        " \n",
        "# extract features from all images\n",
        "directory = 'Flicker8k_Dataset'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open('datalab/features.pkl', 'wb'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "535617536/553467096 [============================>.] - ETA: 1s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "553467904/553467096 [==============================] - 31s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Extracted Features: 8091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zujFYoQmYJKd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1c2eb585-6e80-47ff-87bc-61010fd25b83",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265570421,
          "user_tz": -120,
          "elapsed": 2108,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -la datalab/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 130188\r\n",
            "drwxr-xr-x 1 root root      4096 Jun 29 09:17 .\r\n",
            "drwxr-xr-x 1 root root      4096 Jun 29 07:54 ..\r\n",
            "drwxr-xr-x 4 root root      4096 Jun 25 16:59 .config\r\n",
            "-rw-r--r-- 1 root root 133296870 Jun 29 09:17 features.pkl\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVArfFeiawCR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "56c775cf-dace-4382-a7de-561fa955cc3f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265577205,
          "user_tz": -120,
          "elapsed": 4829,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_text.zip\n",
        "!unzip Flickr8k_text.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-29 09:46:13--  http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_text.zip\n",
            "Resolving nlp.cs.illinois.edu (nlp.cs.illinois.edu)... 192.17.58.132\n",
            "Connecting to nlp.cs.illinois.edu (nlp.cs.illinois.edu)|192.17.58.132|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2340801 (2.2M) [application/zip]\n",
            "Saving to: ‘Flickr8k_text.zip’\n",
            "\n",
            "Flickr8k_text.zip   100%[===================>]   2.23M  2.06MB/s    in 1.1s    \n",
            "\n",
            "2018-06-29 09:46:14 (2.06 MB/s) - ‘Flickr8k_text.zip’ saved [2340801/2340801]\n",
            "\n",
            "Archive:  Flickr8k_text.zip\n",
            "  inflating: CrowdFlowerAnnotations.txt  \n",
            "  inflating: ExpertAnnotations.txt   \n",
            "  inflating: Flickr8k.lemma.token.txt  \n",
            "  inflating: __MACOSX/._Flickr8k.lemma.token.txt  \n",
            "  inflating: Flickr8k.token.txt      \n",
            "  inflating: Flickr_8k.devImages.txt  \n",
            "  inflating: Flickr_8k.testImages.txt  \n",
            "  inflating: Flickr_8k.trainImages.txt  \n",
            "  inflating: readme.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a6stBPkWM91l",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c442c2c0-7aef-4ef4-a37c-fdba4b91fa68",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265580293,
          "user_tz": -120,
          "elapsed": 943,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "filename = 'Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFsInOXuNBQA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3ef633e-d99c-4b43-96b1-8e4487668dc4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265584331,
          "user_tz": -120,
          "elapsed": 1009,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# parse descriptions\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping\n",
        " \n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fdURWJhcNEe5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7e43e16e-4861-44e2-c2c8-84479c75fd5f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265588144,
          "user_tz": -120,
          "elapsed": 1072,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        " \n",
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        " \n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pyjJss0pNIRZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "68b6677f-7fdd-420d-9cb1-76a2289e1d43",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265591395,
          "user_tz": -120,
          "elapsed": 1202,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        " \n",
        "# save descriptions\n",
        "save_descriptions(descriptions, 'datalab/descriptions.txt')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdH3AK_bNSPu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "acfc9c30-1bf9-4a39-c08e-ba9f1b2408b0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265596553,
          "user_tz": -120,
          "elapsed": 2390,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -la datalab/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 133064\r\n",
            "drwxr-xr-x 1 root root      4096 Jun 29 09:46 .\r\n",
            "drwxr-xr-x 1 root root      4096 Jun 29 09:46 ..\r\n",
            "drwxr-xr-x 4 root root      4096 Jun 25 16:59 .config\r\n",
            "-rw-r--r-- 1 root root   2943284 Jun 29 09:46 descriptions.txt\r\n",
            "-rw-r--r-- 1 root root 133296870 Jun 29 09:17 features.pkl\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EQ1RAw0OW8ZZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5f7582e2-a551-4bdc-b062-2017bea80c44",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530265608182,
          "user_tz": -120,
          "elapsed": 1438,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        " \n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # skip empty lines\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "      # get the image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)\n",
        " \n",
        "# load clean descriptions into memory \n",
        "def load_descriptions_from_file(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    # split line by white space\n",
        "    tokens = line.split()\n",
        "    # split id from description\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    # skip images not in the set\n",
        "    if image_id in dataset:\n",
        "      # create list\n",
        "      if image_id not in descriptions:\n",
        "        descriptions[image_id] = list()\n",
        "      # wrap description in tokens\n",
        "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "      # store\n",
        "      descriptions[image_id].append(desc)\n",
        "  return descriptions\n",
        " \n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "  # load all features\n",
        "  all_features = load(open(filename, 'rb'))\n",
        "  # filter features\n",
        "  features = {k: all_features[k] for k in dataset}\n",
        "  return features\n",
        " \n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        " \n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        " \n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)\n",
        " \n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # walk through each description for the image\n",
        "  for desc in desc_list:\n",
        "    # encode the sequence\n",
        "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "    # split one sequence into multiple X,y pairs\n",
        "    for i in range(1, len(seq)):\n",
        "      # split into input and output pair\n",
        "      in_seq, out_seq = seq[:i], seq[i]\n",
        "      # pad input sequence\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "      # encode output sequence\n",
        "      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "      # store\n",
        "      X1.append(photo)\n",
        "      X2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return array(X1), array(X2), array(y)\n",
        " \n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  model.summary()\n",
        "  #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        " \n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "  # loop for ever over images\n",
        "  while 1:\n",
        "    for key, desc_list in descriptions.items():\n",
        "      # retrieve the photo feature\n",
        "      photo = photos[key][0]\n",
        "      in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "      yield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o78pYQoyWjYi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3685
        },
        "outputId": "630d34ea-6ac1-4a31-ae3d-2ee7aed41c25",
        "executionInfo": {
          "status": "error",
          "timestamp": 1530215785204,
          "user_tz": -120,
          "elapsed": 184117,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# load training dataset (6K)\n",
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_descriptions_from_file('datalab/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('datalab/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "maxlength = max_length(train_descriptions)\n",
        "print('Description Length: %d' % maxlength)\n",
        " \n",
        "# define the model\n",
        "model = define_model(vocab_size, maxlength)\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "hists = []\n",
        "for i in range(epochs):\n",
        "  # create the data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, maxlength)\n",
        "  # fit for one epoch\n",
        "  hist = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "  # guardo las perdidas en un historico para ver qué modelo es mejor\n",
        "  hists.append(hist.history['loss'])\n",
        "  # save model\n",
        "  model.save('model_' + str(i) + '.h5')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 34, 256)      1940224     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 34, 256)      0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          1048832     dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          525312      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 256)          0           dense_7[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 7579)         1947803     dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_1/Adam/Variable_15/Assign = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/Adam/Variable_15, training_1/Adam/zeros_15)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1ba7d87a87bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# fit for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;31m# guardo las perdidas en un historico para ver qué modelo es mejor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mhists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_1/Adam/Variable_15/Assign = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/Adam/Variable_15, training_1/Adam/zeros_15)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_1/Adam/Variable_15/Assign', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-1ba7d87a87bd>\", line 28, in <module>\n    hist = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 2080, in fit_generator\n    self._make_train_function()\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 992, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 458, in get_updates\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\", line 458, in <listcomp>\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 695, in zeros\n    return variable(v, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 396, in variable\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 412, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 219, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 60, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_1/Adam/Variable_15/Assign = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_1/Adam/Variable_15, training_1/Adam/zeros_15)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Jmv-243bJKct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No puedo continuar porque me da una ResourceExhaustedException. Por mucho que lo he repetido desde diferentes cuentas sigue dando el mismo fallo. Voy a intentar reducir las capas densas."
      ]
    },
    {
      "metadata": {
        "id": "x9OTMukmJbeX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0faa95f2-4e12-48fc-ab85-a377cebc4f38",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530219847641,
          "user_tz": -120,
          "elapsed": 666,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(128, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(128)(se2)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(128, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  model.summary()\n",
        "  #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y5FV6bFKJflq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1258
        },
        "outputId": "a52a66ef-f797-452a-ed01-c01b1bfd8213",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530237194956,
          "user_tz": -120,
          "elapsed": 17271958,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# load training dataset (6K)\n",
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_descriptions_from_file('datalab/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('datalab/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "maxlength = max_length(train_descriptions)\n",
        "print('Description Length: %d' % maxlength)\n",
        " \n",
        "# define the model\n",
        "model = define_model(vocab_size, maxlength)\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "#steps = 10\n",
        "hists = []\n",
        "for i in range(epochs):\n",
        "  # create the data generator\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, maxlength)\n",
        "  # fit for one epoch\n",
        "  hist = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "  # guardo las perdidas en un historico para ver qué modelo es mejor\n",
        "  hists.append(hist.history['loss'])\n",
        "  # save model\n",
        "  model.save('model_' + str(i) + '.h5')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 34, 128)      970112      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 4096)         0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 34, 128)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          524416      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 128)          131584      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 128)          0           dense_4[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          16512       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 7579)         977691      dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,620,315\n",
            "Trainable params: 2,620,315\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/1\n",
            "  34/6000 [..............................] - ETA: 17:58 - loss: 7.1158"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 876s 146ms/step - loss: 4.7381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 873s 145ms/step - loss: 3.9779\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 876s 146ms/step - loss: 3.7381\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 872s 145ms/step - loss: 3.6042\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 870s 145ms/step - loss: 3.5116\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 883s 147ms/step - loss: 3.4476\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 887s 148ms/step - loss: 3.3994\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 877s 146ms/step - loss: 3.3729\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 853s 142ms/step - loss: 3.3410\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 831s 138ms/step - loss: 3.3189\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 830s 138ms/step - loss: 3.2973\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 839s 140ms/step - loss: 3.2832\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 842s 140ms/step - loss: 3.2719\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 846s 141ms/step - loss: 3.2583\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 879s 147ms/step - loss: 3.2527\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 868s 145ms/step - loss: 3.2432\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 861s 143ms/step - loss: 3.2370\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 875s 146ms/step - loss: 3.2286\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 853s 142ms/step - loss: 3.2250\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 875s 146ms/step - loss: 3.2173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GJc7vOSQX4X3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "424b23a7-8764-4d26-d069-6d9a8351828b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530242464254,
          "user_tz": -120,
          "elapsed": 596,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hists)\n",
        "plt.legend()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdbcbb73be0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4GvgufgPj-9o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "955aab32-ab82-48c4-c635-b99a3f9e58b3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530242534802,
          "user_tz": -120,
          "elapsed": 1644,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1686304\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 01:53 .\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 28 20:49 ..\r\n",
            "drwx------ 4 root root       4096 Jun 28 20:51 .cache\r\n",
            "drwxr-xr-x 3 root root       4096 Jun 28 20:51 .config\r\n",
            "-rw-r--r-- 1 root root    2918552 Oct 14  2013 CrowdFlowerAnnotations.txt\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 28 21:02 datalab\r\n",
            "-rw-r--r-- 1 root root     346674 Oct 14  2013 ExpertAnnotations.txt\r\n",
            "drwxr-xr-x 2 root root     430080 Oct  3  2012 Flicker8k_Dataset\r\n",
            "-rw-r--r-- 1 root root 1115419746 Oct 24  2013 Flickr8k_Dataset.zip\r\n",
            "-rw-r--r-- 1 root root      25801 Oct 10  2013 Flickr_8k.devImages.txt\r\n",
            "-rw-r--r-- 1 root root    3244761 Feb 16  2012 Flickr8k.lemma.token.txt\r\n",
            "-rw-r--r-- 1 root root      25775 Oct 10  2013 Flickr_8k.testImages.txt\r\n",
            "-rw-r--r-- 1 root root    2340801 Oct 28  2013 Flickr8k_text.zip\r\n",
            "-rw-r--r-- 1 root root    3395237 Oct 14  2013 Flickr8k.token.txt\r\n",
            "-rw-r--r-- 1 root root     154678 Oct 10  2013 Flickr_8k.trainImages.txt\r\n",
            "drwxr-xr-x 4 root root       4096 Jun 28 20:50 .forever\r\n",
            "drwxr-xr-x 5 root root       4096 Jun 28 20:51 .ipython\r\n",
            "drwxr-xr-x 3 root root       4096 Jun 28 20:54 .keras\r\n",
            "drwx------ 3 root root       4096 Jun 28 20:51 .local\r\n",
            "drwxrwxr-x 3 root root       4096 Jun 28 20:53 __MACOSX\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 23:44 model_10.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 23:58 model_11.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 00:12 model_12.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 00:26 model_13.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 00:41 model_14.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 00:55 model_15.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 01:09 model_16.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 01:24 model_17.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 01:38 model_18.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 29 01:53 model_19.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 21:34 model_1.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 21:49 model_2.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 22:03 model_3.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 22:18 model_4.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 22:32 model_5.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 22:47 model_6.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 23:02 model_7.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 23:16 model_8.h5\r\n",
            "-rw-r--r-- 1 root root   31490604 Jun 28 23:30 model_9.h5\r\n",
            "drwx------ 3 root root       4096 Jun 28 20:54 .nv\r\n",
            "-rw-r--r-- 1 root root       1821 Oct 14  2013 readme.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "If303gSqD22N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Evaluar el modelo: ** Después de estar entrenando el modelo unas 5 horas guardé el archivo model_19.h5 para poder cargarlo posteriormente y evaluar el modelo... pero no ha podido ser, porque al cargarlo usando las líneas que comentaba antes:\n",
        "```\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "```\n",
        "me da un error\n",
        "```\n",
        "[object CloseEvent]\n",
        "```\n",
        "Y se desconecta el entorno. Así que he entrenado de nuevo sólo una época, aunque he dejado el código tal cual sería si cogiera la época 20. La evaluación del modelo (BLEU y la nueva predicción a partir de una foto nueva) la he hecho con la primera época, de ahí los resultados."
      ]
    },
    {
      "metadata": {
        "id": "PkBnFP0QDjyl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "530b4901-4dc4-4bb5-8597-c20d1f3ea928",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530267757744,
          "user_tz": -120,
          "elapsed": 48592,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "  \n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        " \n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        " \n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        " \n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        " \n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " \n",
        "# prepare tokenizer on train set\n",
        " \n",
        "# load training dataset (6K)\n",
        "filename = 'Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_descriptions_from_file('datalab/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "# save the tokenizer: Para usarlo después cuando generemos nuevas descripciones a partir del modelo\n",
        "dump(tokenizer, open('datalab/tokenizer.pkl', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "maxlength = max_length(train_descriptions)\n",
        "print('Description Length: %d' % maxlength)\n",
        " \n",
        "# prepare test set\n",
        " \n",
        "# load test set\n",
        "filename = 'Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_descriptions_from_file('datalab/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('datalab/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        " \n",
        "# load the model\n",
        "# filename = 'model-ep002-loss3.245-val_loss3.612.h5'\n",
        "filename = 'model_19.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, maxlength)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "BLEU-1: 0.574492\n",
            "BLEU-2: 0.313153\n",
            "BLEU-3: 0.206707\n",
            "BLEU-4: 0.090890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iZ-7_sOZFqIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Usar el modelo para generar nuevas descripciones ** "
      ]
    },
    {
      "metadata": {
        "id": "EYUXMbGZElMh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "3d521027-9d16-4b43-e6b9-c28333bad694",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530267883635,
          "user_tz": -120,
          "elapsed": 22657,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3ea4a676-a898-4d7a-a51c-2d12e04ef263\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3ea4a676-a898-4d7a-a51c-2d12e04ef263\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving example.jpg to example.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ORcpGjq9EtXL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "179b9252-d036-4a8d-bb8f-fda9e9251312",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530267892644,
          "user_tz": -120,
          "elapsed": 2062,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1102268\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 10:24 .\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 07:50 ..\r\n",
            "drwx------ 4 root root       4096 Jun 29 07:51 .cache\r\n",
            "drwxr-xr-x 3 root root       4096 Jun 29 07:51 .config\r\n",
            "-rw-r--r-- 1 root root    2918552 Oct 14  2013 CrowdFlowerAnnotations.txt\r\n",
            "drwxr-xr-x 1 root root       4096 Jun 29 10:21 datalab\r\n",
            "-rw-r--r-- 1 root root     315475 Jun 29 10:24 example.jpg\r\n",
            "-rw-r--r-- 1 root root     346674 Oct 14  2013 ExpertAnnotations.txt\r\n",
            "drwxr-xr-x 2 root root     450560 Oct  3  2012 Flicker8k_Dataset\r\n",
            "-rw-r--r-- 1 root root 1115419746 Oct 24  2013 Flickr8k_Dataset.zip\r\n",
            "-rw-r--r-- 1 root root      25801 Oct 10  2013 Flickr_8k.devImages.txt\r\n",
            "-rw-r--r-- 1 root root    3244761 Feb 16  2012 Flickr8k.lemma.token.txt\r\n",
            "-rw-r--r-- 1 root root      25775 Oct 10  2013 Flickr_8k.testImages.txt\r\n",
            "-rw-r--r-- 1 root root    2340801 Oct 28  2013 Flickr8k_text.zip\r\n",
            "-rw-r--r-- 1 root root    3395237 Oct 14  2013 Flickr8k.token.txt\r\n",
            "-rw-r--r-- 1 root root     154678 Oct 10  2013 Flickr_8k.trainImages.txt\r\n",
            "drwxr-xr-x 4 root root       4096 Jun 29 07:51 .forever\r\n",
            "drwxr-xr-x 5 root root       4096 Jun 29 07:51 .ipython\r\n",
            "drwxr-xr-x 3 root root       4096 Jun 29 07:54 .keras\r\n",
            "drwx------ 3 root root       4096 Jun 29 07:51 .local\r\n",
            "drwxrwxr-x 3 root root       4096 Jun 29 09:46 __MACOSX\r\n",
            "-rw-r--r-- 1 root root       1821 Oct 14  2013 readme.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TBpo2EdMEwxu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "019a4d94-5449-41b3-a3e0-00d59d4627b0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530267916518,
          "user_tz": -120,
          "elapsed": 2050,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open('datalab/tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "maxlength = 34"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTR2VykaE4Uu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b622e27-9bec-4717-d89d-73327684bc23",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530267936784,
          "user_tz": -120,
          "elapsed": 7049,
          "user": {
            "displayName": "Raquel Ventas",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110670086389234427354"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def extract_features_one_image(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature\n",
        " \n",
        "# load and prepare the photograph\n",
        "photo = extract_features_one_image('Flicker8k_Dataset/1084040636_97d9633581.jpg')\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, maxlength)\n",
        "print(description)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq two dogs are playing in the grass endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JxHrdS7HFzGs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Detecta que hay dos perros, tiene cierto sentido porque la imágen del reflejo confunde a la red :-)\n",
        "\n",
        "![texto alternativo](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/example.jpg)\n",
        "\n",
        "Me quedo con las ganas de poder obtener mejores resultados, he encontrado bastantes dificultades con el Colab a la hora de avanzar."
      ]
    }
  ]
}